from google import genai
from google.genai import types
import pandas as pd
import json
import os
import sys
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import glob
import os
from pathlib import Path
from token_cost_calculator import TokenCostCalculator

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
API_KEY = os.getenv('HACKATHON_GEMINI_API_KEY')
if not API_KEY:
    raise ValueError("HACKATHON_GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”. ~/.zshrcì— 'export HACKATHON_GEMINI_API_KEY=\"your_api_key\"'ë¥¼ ì¶”ê°€í•˜ê³  'source ~/.zshrc'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

client = genai.Client(api_key=API_KEY)

def find_translation_review_files(base_path):
    """ì§€ì •ëœ ê²½ë¡œì—ì„œ ëª¨ë“  translation_review.csv íŒŒì¼ì„ ì°¾ì•„ ë°˜í™˜"""
    csv_files = []
    base_path = Path(base_path)
    
    # glob íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ translation_review.csv íŒŒì¼ ì°¾ê¸°
    pattern = base_path / "**/translation_review.csv"
    
    for csv_file in base_path.glob("**/translation_review.csv"):
        if csv_file.is_file():
            csv_files.append(str(csv_file))
    
    return sorted(csv_files)

def calculate_total_textboxes_from_xlsx(csv_file_path):
    """
    CSV íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹í•˜ëŠ” ì›ë³¸ XLSX íŒŒì¼ì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë°•ìŠ¤ ìˆ˜ë¥¼ ê³„ì‚°
    
    Args:
        csv_file_path (str): translation_review.csv íŒŒì¼ ê²½ë¡œ
        
    Returns:
        int: ì „ì²´ ê³ ìœ  í…ìŠ¤íŠ¸ë°•ìŠ¤ ìˆ˜
    """
    try:
        # CSV íŒŒì¼ ê²½ë¡œì—ì„œ ì—í”¼ì†Œë“œ ë²ˆí˜¸ ì¶”ì¶œ
        # ì˜ˆ: .../preprocessed/1/translation_review.csv -> ep_1.xlsx
        csv_path = Path(csv_file_path)
        episode_dir = csv_path.parent.name  # "1", "2", "3" ë“±
        
        # preprocessed ë””ë ‰í† ë¦¬ë¡œ ì´ë™
        preprocessed_dir = csv_path.parent.parent
        
        # ì›ë³¸ XLSX íŒŒì¼ ê²½ë¡œ êµ¬ì„±
        xlsx_file_path = preprocessed_dir / f"ep_{episode_dir}.xlsx"
        
        if not xlsx_file_path.exists():
            print(f"ê²½ê³ : ì›ë³¸ XLSX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {xlsx_file_path}")
            return 0
        
        # XLSX íŒŒì¼ ì½ê¸°
        df = pd.read_excel(xlsx_file_path)
        
        # file_nameê³¼ text_box_order ì¹¼ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'file_name' in df.columns and 'text_box_order' in df.columns:
            # ê³ ìœ í•œ í…ìŠ¤íŠ¸ë°•ìŠ¤ ìˆ˜ ê³„ì‚° (3_xlsx_comparison.py ë°©ì‹ê³¼ ë™ì¼)
            unique_textboxes = df.groupby(['file_name', 'text_box_order']).size()
            total_unique_textboxes = len(unique_textboxes)
        else:
            # ì¹¼ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ì „ì²´ í–‰ ìˆ˜ ì‚¬ìš©
            total_unique_textboxes = len(df)
        
        print(f"ğŸ“Š {xlsx_file_path.name}ì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë°•ìŠ¤ ìˆ˜: {total_unique_textboxes:,}ê°œ")
        return total_unique_textboxes
        
    except Exception as e:
        print(f"ì˜¤ë¥˜: XLSX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë°•ìŠ¤ ìˆ˜ ê³„ì‚° ì‹¤íŒ¨ ({csv_file_path}): {e}")
        return 0

def process_single_csv_file(csv_file_path, guideline_path=None, model_name="gemini-2.5-pro", max_workers=10):
    """ë‹¨ì¼ CSV íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ LLM ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±"""
    print(f"\n{'='*60}")
    print(f"íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {csv_file_path}")
    print(f"{'='*60}")
    
    try:
        # CSV íŒŒì¼ ì½ê¸°
        test_data = pd.read_csv(csv_file_path)
        
        print(f"ë°ì´í„° êµ¬ì¡° í™•ì¸:")
        print(f"ì „ì²´ í–‰ ìˆ˜: {len(test_data)}")
        print(f"ì¹¼ëŸ¼ëª…: {test_data.columns.tolist()}")
        
        # targetê³¼ val ì¹¼ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if 'target' not in test_data.columns or 'val' not in test_data.columns:
            print(f"ê²½ê³ : '{csv_file_path}'ì—ì„œ 'target' ë˜ëŠ” 'val' ì¹¼ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì¹¼ëŸ¼: {test_data.columns.tolist()}")
            return None
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° êµ¬ì¡° ì„¤ê³„
        batch_size = 20
        comparison_data = []
        
        # ë°°ì¹˜ë³„ë¡œ ë°ì´í„° ë¶„í• 
        for i in range(0, len(test_data), batch_size):
            batch = test_data[i:i+batch_size]
            batch_dict = {
                'batch_id': i // batch_size + 1,
                'start_index': i,
                'end_index': min(i + batch_size, len(test_data)),
                'data': [],
                'source_file': csv_file_path
            }
            
            for idx, row in batch.iterrows():
                batch_dict['data'].append({
                    'index': idx,
                    'target': row['target'],
                    'val': row['val'],
                    'file_name': row.get('file_name', ''),
                    'text_box_order': row.get('text_box_order', '')
                })
            
            comparison_data.append(batch_dict)
        
        print(f"ì´ {len(comparison_data)}ê°œì˜ ë°°ì¹˜ë¡œ ë¶„í• ë¨ (ê° ë°°ì¹˜ë‹¹ ìµœëŒ€ {batch_size}ê°œ í•­ëª©)")
        
        # ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬)
        all_results = process_all_batches_for_single_file(comparison_data, csv_file_path, guideline_path, model_name, max_workers)
        
        # ê²°ê³¼ë¥¼ ì›ë³¸ íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì— ì €ì¥
        save_results_to_original_location(all_results, comparison_data, csv_file_path)
        
        return {
            'source_file': csv_file_path,
            'total_batches': len(comparison_data),
            'results': all_results,
            'success': True
        }
        
    except Exception as e:
        print(f"íŒŒì¼ '{csv_file_path}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            'source_file': csv_file_path,
            'error': str(e),
            'success': False
        }

# LLMìœ¼ë¡œ ë¹„êµ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜ (ì‚¬ìš©ì ì§€ì • JSON í˜•ì‹)
def analyze_batch_differences(batch_data, guideline_path=None, cost_calculator=None):
    """ë°°ì¹˜ ë°ì´í„°ì˜ targetê³¼ valì„ ë¹„êµí•˜ì—¬ ìˆ˜ì • ì‚¬ìœ ë¥¼ ë¶„ì„ (ì‚¬ìš©ì ì§€ì • JSON ì…ì¶œë ¥)"""
    
    # ê°€ì´ë“œë¼ì¸ íŒŒì¼ ì½ê¸°
    guideline_content = "ê°€ì´ë“œë¼ì¸ íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    if guideline_path and os.path.exists(guideline_path):
        try:
            with open(guideline_path, 'r', encoding='utf-8') as f:
                guideline_content = f.read()
            # print(f"ğŸ“‹ ê°€ì´ë“œë¼ì¸ íŒŒì¼ ë¡œë“œë¨: {guideline_path}")
        except Exception as e:
            print(f"ê²½ê³ : ê°€ì´ë“œë¼ì¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({guideline_path}): {e}")
            guideline_content = "ê°€ì´ë“œë¼ì¸ íŒŒì¼ ì½ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    elif guideline_path:
        print(f"ê²½ê³ : ê°€ì´ë“œë¼ì¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {guideline_path}")
    else:
        print("ê²½ê³ : ê°€ì´ë“œë¼ì¸ íŒŒì¼ ê²½ë¡œê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê°€ì´ë“œë¼ì¸ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # ì‚¬ìš©ì ì§€ì • JSON ì…ë ¥ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
    json_input = []
    
    for item in batch_data['data']:
        json_input.append({
            "translated_text": item['target'],
            "edited_text": item['val']
        })
    
    # JSON ì…ë ¥ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    json_input_str = json.dumps(json_input, ensure_ascii=False, indent=2)
    
    # ì‚¬ìš©ì ì œê³µ í”„ë¡¬í”„íŠ¸ë¡œ LLMì—ê²Œ ë¶„ì„ ìš”ì²­ (ê°€ì´ë“œë¼ì¸ í¬í•¨)
    prompt = f"""í´ë¼ì´ì–¸íŠ¸ ê°€ì´ë“œë¼ì¸:
{guideline_content}

---

ë‹¤ìŒ JSON í˜•ì‹ì˜ ì…ë ¥ ë°ì´í„°ì— í¬í•¨ëœ ê° ë²ˆì—­ ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

{json_input_str}


ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì˜ JSON ë°°ì—´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

[
  {{
    "translated_text": "ì…ë ¥ë°›ì€ ë²ˆì—­ë¬¸ ê·¸ëŒ€ë¡œ",
    "edited_text": "ì…ë ¥ë°›ì€ ê²€ìˆ˜ë³¸ ê·¸ëŒ€ë¡œ",
    "tag": ["ì„ íƒëœ íƒœê·¸ë“¤"],
    "comment": "êµ¬ì²´ì ì¸ ë¶„ì„ ì½”ë©˜íŠ¸"
  }}
]

ìœ íš¨í•œ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
    
    # system instruction íŒŒì¼ ì½ê¸°
    system_instruction_path = "../data/prompt/comment_generation_prompt.txt"
    system_instruction_content = ""
    
    if os.path.exists(system_instruction_path):
        try:
            with open(system_instruction_path, 'r', encoding='utf-8') as f:
                system_instruction_content = f.read()
            # print(f"ğŸ“‹ System instruction íŒŒì¼ ë¡œë“œë¨: {system_instruction_path}")
        except Exception as e:
            print(f"ê²½ê³ : System instruction íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({system_instruction_path}): {e}")
            system_instruction_content = "System instruction íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
    else:
        print(f"ê²½ê³ : System instruction íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {system_instruction_path}")
        system_instruction_content = "System instruction íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
    
    response = client.models.generate_content(
        model="gemini-2.5-pro",
        config=types.GenerateContentConfig(
            system_instruction=system_instruction_content),
        contents=prompt
    )
    
    # í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ê³„ì‚°
    cost_info = None
    if cost_calculator:
        cost_info = cost_calculator.calculate_batch_cost(response)
        cost_calculator.print_batch_cost(batch_data['batch_id'], cost_info)
    
    # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
    try:
        # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json íƒœê·¸ ì œê±°)
        response_text = response.text.strip()
        if response_text.startswith('```json'):
            response_text = response_text[7:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]
        
        parsed_response = json.loads(response_text.strip())
        
        # ì‘ë‹µì´ ë°°ì—´ì¸ì§€ í™•ì¸
        if not isinstance(parsed_response, list):
            raise ValueError("ì‘ë‹µì´ JSON ë°°ì—´ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        
        result = {
            'batch_id': batch_data['batch_id'],
            'input_data': json_input,
            'analysis_result': parsed_response,
            'processed_items': len(batch_data['data']),
            'parsing_success': True
        }
        
        # ë¹„ìš© ì •ë³´ ì¶”ê°€
        if cost_info:
            result['cost_info'] = cost_info
        
        return result
        
    except (json.JSONDecodeError, ValueError) as e:
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        print(f"ì›ë³¸ ì‘ë‹µ: {response.text}")
        
        result = {
            'batch_id': batch_data['batch_id'],
            'input_data': json_input,
            'analysis_result': {
                'error': 'JSON íŒŒì‹± ì‹¤íŒ¨',
                'raw_response': response.text
            },
            'processed_items': len(batch_data['data']),
            'parsing_success': False
        }
        
        # ë¹„ìš© ì •ë³´ ì¶”ê°€ (íŒŒì‹± ì‹¤íŒ¨í•´ë„ í† í°ì€ ì‚¬ìš©ë¨)
        if cost_info:
            result['cost_info'] = cost_info
        
        return result

# JSON ë¶„ì„ ê²°ê³¼ íŒŒì‹± ë° í™œìš©ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def extract_tags_statistics(all_results):
    """ëª¨ë“  ë¶„ì„ ê²°ê³¼ì—ì„œ íƒœê·¸ë³„ í†µê³„ ì¶”ì¶œ (ìƒˆë¡œìš´ JSON í˜•ì‹ ëŒ€ì‘)"""
    tag_counts = {}
    
    for result in all_results:
        if not result.get('parsing_success', False):
            continue
            
        analysis_results = result.get('analysis_result', [])
        
        # ìƒˆë¡œìš´ í˜•ì‹: ë°°ì—´ì˜ ê° í•­ëª©ì—ì„œ íƒœê·¸ ì¶”ì¶œ
        for item in analysis_results:
            # íƒœê·¸ í†µê³„
            for tag in item.get('tag', []):
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
    
    return {
        'tag_counts': tag_counts
    }

def generate_analysis_report(all_results, total_textboxes=None):
    """ì „ì²´ ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„± (ìƒˆë¡œìš´ JSON í˜•ì‹ ëŒ€ì‘)"""
    if not all_results:
        return "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    stats = extract_tags_statistics(all_results)
    total_pairs = sum(result.get('processed_items', 0) for result in all_results)
    successful_batches = sum(1 for result in all_results if result.get('parsing_success', False))
    
    report = f"""
=== ë²ˆì—­ í’ˆì§ˆ ë¶„ì„ ë¦¬í¬íŠ¸ ===

ğŸ“Š ì „ì²´ í†µê³„:
- ì²˜ë¦¬ëœ ì´ ë°°ì¹˜ ìˆ˜: {len(all_results)}
- ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ëœ ë°°ì¹˜: {successful_batches}
- ì²˜ë¦¬ëœ ì´ ìŒ ìˆ˜: {total_pairs}"""
    
    if total_textboxes is not None:
        report += f"\n- ì „ì²´ ì‹¤ì§ˆ í…ìŠ¤íŠ¸ë°•ìŠ¤ ìˆ˜: {total_textboxes:,}ê°œ"
    
    report += "\n\nğŸ·ï¸ íƒœê·¸ë³„ ë°œìƒ ë¹ˆë„:"
    
    # íƒœê·¸ë³„ í†µê³„ ì¶œë ¥ (ë¹ˆë„ìˆœ)
    sorted_tags = sorted(stats['tag_counts'].items(), key=lambda x: x[1], reverse=True)
    for tag, count in sorted_tags:
        if total_textboxes is not None and total_textboxes > 0:
            # ì „ì²´ í…ìŠ¤íŠ¸ë°•ìŠ¤ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ í¼ì„¼íŠ¸ ê³„ì‚°
            percentage = (count / total_textboxes) * 100
            report += f"\n  - {tag}: {count}íšŒ ({percentage:.1f}%)"
        else:
            # ê¸°ì¡´ ë°©ì‹: ì²˜ë¦¬ëœ ì´ ìŒ ìˆ˜ ê¸°ì¤€
            percentage = (count / total_pairs) * 100 if total_pairs > 0 else 0
            report += f"\n  - {tag}: {count}íšŒ ({percentage:.1f}%)"
    
    return report

def export_structured_results(all_results, comparison_data, output_dir="analysis_results"):
    """êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì—¬ëŸ¬ í˜•íƒœë¡œ ë‚´ë³´ë‚´ê¸°"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. ì „ì²´ JSON ê²°ê³¼ ì €ì¥
    full_results_path = os.path.join(output_dir, f"full_analysis_{timestamp}.json")
    with open(full_results_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    # 2. ìš”ì•½ í†µê³„ JSON ì €ì¥
    stats = extract_tags_statistics(all_results)
    stats_path = os.path.join(output_dir, f"statistics_{timestamp}.json")
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # 3. í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥
    report = generate_analysis_report(all_results)
    report_path = os.path.join(output_dir, f"report_{timestamp}.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # 4. CSV í˜•íƒœë¡œ ê° ìŒë³„ ê²°ê³¼ ì €ì¥ (ì‚¬ìš©ì ì§€ì • ì¹¼ëŸ¼ëª…)
    csv_data = []
    
    # ë°°ì¹˜ë³„ë¡œ ì›ë³¸ ë°ì´í„°ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ë§¤ì¹­
    for batch in comparison_data:
        batch_id = batch['batch_id']
        original_items = batch['data']
        
        # í•´ë‹¹ ë°°ì¹˜ì˜ ë¶„ì„ ê²°ê³¼ ì°¾ê¸°
        batch_analysis = None
        for result in all_results:
            if result['batch_id'] == batch_id:
                batch_analysis = result
                break
        
        if not batch_analysis:
            # ë¶„ì„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ì›ë³¸ ë°ì´í„°ëŠ” ë³´ì¡´
            for original_item in original_items:
                csv_row = {
                    'file_name': original_item.get('file_name', ''),
                    'text_box_order': original_item.get('text_box_order', ''),
                    'target': original_item.get('target', ''),
                    'val': original_item.get('val', ''),
                    'tag': '',
                    'comment': 'ë¶„ì„ ì‹¤íŒ¨'
                }
                csv_data.append(csv_row)
            continue
        
        analysis_results = batch_analysis.get('analysis_result', []) if batch_analysis.get('parsing_success', False) else []
        
        # ì›ë³¸ ë°ì´í„°ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ìˆœì„œëŒ€ë¡œ ë§¤ì¹­
        for i, original_item in enumerate(original_items):
            analysis_item = analysis_results[i] if i < len(analysis_results) else {}
            
            csv_row = {
                'file_name': original_item.get('file_name', ''),
                'text_box_order': original_item.get('text_box_order', ''),
                'target': original_item.get('target', ''),
                'val': original_item.get('val', ''),
                'tag': ', '.join(analysis_item.get('tag', [])) if analysis_item else '',
                'comment': analysis_item.get('comment', '') if analysis_item else 'ë¶„ì„ ê²°ê³¼ ì—†ìŒ'
            }
            csv_data.append(csv_row)
    
    if csv_data:
        import pandas as pd
        df = pd.DataFrame(csv_data)
        csv_path = os.path.join(output_dir, f"pair_analysis_{timestamp}.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        print(f"ë¶„ì„ ê²°ê³¼ê°€ ë‹¤ìŒ íŒŒì¼ë“¤ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"  - ì „ì²´ ê²°ê³¼: {full_results_path}")
        print(f"  - í†µê³„ ìš”ì•½: {stats_path}")
        print(f"  - í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸: {report_path}")
        print(f"  - CSV ê²°ê³¼: {csv_path}")
    
    return {
        'full_results': full_results_path,
        'statistics': stats_path,
        'report': report_path,
        'csv': csv_path if csv_data else None
    }

# ë°ì´í„° ì €ì¥ ë° ê´€ë¦¬ë¥¼ ìœ„í•œ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def save_individual_batch_result(batch_result, batch_data, output_dir="batch_results"):
    """ê°œë³„ ë°°ì¹˜ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    batch_id = batch_result['batch_id']
    filename = f"batch_{batch_id}_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)
    
    # ì €ì¥í•  ë°ì´í„° êµ¬ì„±
    save_data = {
        'batch_info': {
            'batch_id': batch_id,
            'timestamp': timestamp,
            'processed_items': batch_result['processed_items'],
            'parsing_success': batch_result.get('parsing_success', False)
        },
        'input_data': batch_result.get('input_data', []),
        'analysis_result': batch_result.get('analysis_result', []),
        'cost_info': batch_result.get('cost_info', {}),
        'original_batch_data': {
            'start_index': batch_data['start_index'],
            'end_index': batch_data['end_index'],
            'data_preview': [
                {
                    'index': item['index'],
                    'file_name': item.get('file_name', ''),
                    'text_box_order': item.get('text_box_order', ''),
                    'target': item['target'][:100] + '...' if len(item['target']) > 100 else item['target'],
                    'val': item['val'][:100] + '...' if len(item['val']) > 100 else item['val']
                }
                for item in batch_data['data'][:3]  # ì²˜ìŒ 3ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
            ]
        }
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ë°°ì¹˜ {batch_id} ê²°ê³¼ ì €ì¥: {filename}")
    return filepath

def save_cost_summary_to_file(cost_calculator, all_results, output_dir="analysis_results"):
    """í† í° ë¹„ìš© ìš”ì•½ì„ txt íŒŒì¼ë¡œ ì €ì¥"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"token_cost_summary_{timestamp}.txt"
    filepath = os.path.join(output_dir, filename)
    
    # ì „ì²´ ë¹„ìš© ìš”ì•½ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    summary = cost_calculator.get_total_cost_summary()
    
    # ë°°ì¹˜ë³„ ë¹„ìš© ì •ë³´ ìˆ˜ì§‘
    batch_costs = []
    for result in all_results:
        if 'cost_info' in result:
            batch_costs.append({
                'batch_id': result['batch_id'],
                'cost_info': result['cost_info'],
                'parsing_success': result.get('parsing_success', False)
            })
    
    # txt íŒŒì¼ ë‚´ìš© ìƒì„±
    content = f"""í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ë¶„ì„ ë¦¬í¬íŠ¸
ìƒì„± ì‹œê°„: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}
{'='*60}

ğŸ“Š ì „ì²´ ìš”ì•½
{'='*60}
ì´ ë°°ì¹˜ ìˆ˜: {len(all_results)}
ì„±ê³µí•œ ë°°ì¹˜: {sum(1 for r in all_results if r.get('parsing_success', False))}
ì‹¤íŒ¨í•œ ë°°ì¹˜: {sum(1 for r in all_results if not r.get('parsing_success', False))}

ğŸ’° ì „ì²´ í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© [{summary.get('model_name', 'Unknown')} - {summary.get('pricing_tier', 'Unknown')}]
{'='*60}
ğŸ“¥ ì´ ì…ë ¥ í† í°: {summary['total_input_tokens']:,} í† í°
ğŸ“¤ ì´ ì¶œë ¥ í† í°: {summary['total_output_tokens']:,} í† í°
ğŸ¤” ì´ ì‚¬ê³  í† í°: {summary['total_thinking_tokens']:,} í† í°
ğŸ’¾ ì´ ìºì‹œ í† í°: {summary.get('total_cached_tokens', 0):,} í† í°
ğŸ“Š ì´ í† í° ìˆ˜: {summary['total_tokens']:,} í† í°

ğŸ’µ ë¹„ìš© ì„¸ë¶€ ë‚´ì—­:
   - ì…ë ¥ í† í° ë¹„ìš©: ${summary['total_input_cost']:.6f}
   - ì¶œë ¥ í† í° ë¹„ìš©: ${summary['total_output_cost']:.6f}
   - ì‚¬ê³  í† í° ë¹„ìš©: ${summary['total_thinking_cost']:.6f}
   - ìºì‹œ í† í° ë¹„ìš©: ${summary.get('total_cached_cost', 0):.6f}
   - ì´ ë¹„ìš©: ${summary['total_cost']:.6f}
   - ì´ ë¹„ìš© (ì›í™”): â‚©{summary['total_cost'] * 1400:.0f} (í™˜ìœ¨ 1,400ì› ê¸°ì¤€)

ğŸ“ˆ ë°°ì¹˜ë³„ ìƒì„¸ ë¹„ìš©
{'='*60}
"""
    
    # ë°°ì¹˜ë³„ ë¹„ìš© ì •ë³´ ì¶”ê°€
    for batch_cost in batch_costs:
        cost_info = batch_cost['cost_info']
        status = "âœ… ì„±ê³µ" if batch_cost['parsing_success'] else "âŒ ì‹¤íŒ¨"
        estimated_text = " (ì¶”ì •)" if cost_info.get('estimated', False) else ""
        
        tier_info = f" [{cost_info.get('pricing_tier', 'Unknown')}]"
        content += f"""
ë°°ì¹˜ {batch_cost['batch_id']} - {status}{estimated_text}{tier_info}
   ğŸ“¥ ì…ë ¥: {cost_info.get('input_tokens', 0):,} í† í° (${cost_info.get('input_cost', 0):.6f})
   ğŸ“¤ ì¶œë ¥: {cost_info.get('output_tokens', 0):,} í† í° (${cost_info.get('output_cost', 0):.6f})"""
        
        if cost_info.get('thinking_tokens', 0) > 0:
            content += f"\n   ğŸ¤” ì‚¬ê³ : {cost_info.get('thinking_tokens', 0):,} í† í° (${cost_info.get('thinking_cost', 0):.6f})"
        
        if cost_info.get('cached_tokens', 0) > 0:
            content += f"\n   ğŸ’¾ ìºì‹œ: {cost_info.get('cached_tokens', 0):,} í† í° (${cost_info.get('cached_cost', 0):.6f})"
        
        content += f"\n   ğŸ’µ ë°°ì¹˜ ë¹„ìš©: ${cost_info.get('batch_cost', 0):.6f} (â‚©{cost_info.get('batch_cost', 0) * 1400:.2f})\n"
    
    # ê°€ê²© ì •ë³´ ì¶”ê°€ (ëª¨ë¸ë³„ ë™ì  í‘œì‹œ)
    model_name = summary.get('model_name', 'Unknown')
    content += f"""
{'='*60}
ğŸ“‹ ê°€ê²© ì •ë³´ ({model_name})
{'='*60}"""
    
    if model_name == "gemini-2.5-flash":
        content += f"""
ì…ë ¥ í† í°: $0.30 / 1M í† í°
ì¶œë ¥ í† í°: $2.50 / 1M í† í°  
ì‚¬ê³  í† í°: $2.50 / 1M í† í°
ìºì‹œ í† í°: $0.075 / 1M í† í°"""
    elif model_name == "gemini-2.5-pro":
        content += f"""
ì…ë ¥ í† í° (20ë§Œ ë¯¸ë§Œ): $1.25 / 1M í† í°
ì…ë ¥ í† í° (20ë§Œ ì´ìƒ): $2.50 / 1M í† í°
ì¶œë ¥ í† í° (20ë§Œ ë¯¸ë§Œ): $10.0 / 1M í† í°
ì¶œë ¥ í† í° (20ë§Œ ì´ìƒ): $15.0 / 1M í† í°
ì‚¬ê³  í† í°: $10.0 / 1M í† í°
ìºì‹œ í† í° (20ë§Œ ë¯¸ë§Œ): $0.31 / 1M í† í°
ìºì‹œ í† í° (20ë§Œ ì´ìƒ): $0.625 / 1M í† í°
ì„ê³„ê°’: 200,000 í† í°"""
    
    
    content += "\n"
    
    # íŒŒì¼ì— ì €ì¥
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"ğŸ’° í† í° ë¹„ìš© ìš”ì•½ ì €ì¥: {filename}")
    return filepath

def save_batch_results(results, output_dir="analysis_results"):
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"batch_analysis_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    
    print(f"ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    return filepath

def process_batch_with_cost_tracking(batch, guideline_path, cost_calculator, save_individual_batches=True):
    """ë‹¨ì¼ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³  ë¹„ìš©ì„ ì¶”ì í•˜ëŠ” ë˜í¼ í•¨ìˆ˜"""
    try:
        result = analyze_batch_differences(batch, guideline_path, cost_calculator)
        
        # íŒŒì‹± ì„±ê³µ ì—¬ë¶€ í‘œì‹œ
        status = "âœ… ì„±ê³µ" if result.get('parsing_success', False) else "âŒ JSON íŒŒì‹± ì‹¤íŒ¨"
        print(f"ë°°ì¹˜ {batch['batch_id']} ì²˜ë¦¬ ì™„ë£Œ - {status}")
        
        # ê°œë³„ ë°°ì¹˜ ê²°ê³¼ ì €ì¥
        if save_individual_batches:
            save_individual_batch_result(result, batch)
        
        return result
        
    except Exception as e:
        print(f"ë°°ì¹˜ {batch['batch_id']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def process_all_batches_for_single_file(comparison_data, source_file_path, guideline_path=None, model_name="gemini-2.5-pro", max_workers=10):
    """ë‹¨ì¼ íŒŒì¼ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
    all_results = []
    cost_calculator = TokenCostCalculator(model_name=model_name)
    
    # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½
    cost_lock = threading.Lock()
    
    print(f"ì´ {len(comparison_data)}ê°œ ë°°ì¹˜ë¥¼ {max_workers}ê°œ ì›Œì»¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
    
    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ê° ë°°ì¹˜ì— ëŒ€í•´ ì‘ì—… ì œì¶œ (ê°œë³„ ë°°ì¹˜ ì €ì¥ ë¹„í™œì„±í™”)
        future_to_batch = {
            executor.submit(process_batch_with_cost_tracking, batch, guideline_path, cost_calculator, False): batch 
            for batch in comparison_data
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë“¤ì„ ìˆ˜ì§‘
        completed_count = 0
        for future in as_completed(future_to_batch):
            batch = future_to_batch[future]
            completed_count += 1
            
            try:
                result = future.result()
                if result is not None:
                    with cost_lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½
                        all_results.append(result)
                
                print(f"ì§„í–‰ë¥ : {completed_count}/{len(comparison_data)} ë°°ì¹˜ ì™„ë£Œ")
                
            except Exception as e:
                print(f"ë°°ì¹˜ {batch['batch_id']} ê²°ê³¼ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ë°°ì¹˜ ID ìˆœì„œëŒ€ë¡œ ì •ë ¬
    all_results.sort(key=lambda x: x.get('batch_id', 0))
    
    print(f"íŒŒì¼ '{Path(source_file_path).name}' ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(all_results)}ê°œ ë°°ì¹˜ ì²˜ë¦¬ë¨")
    
    # ì „ì²´ ë¹„ìš© ìš”ì•½ ì¶œë ¥
    cost_calculator.print_total_cost_summary()
    
    return all_results

def process_all_batches(comparison_data, save_results=True, use_structured_export=True, save_individual_batches=True, model_name="gemini-2.5-pro", max_workers=10):
    """ëª¨ë“  ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥ (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)"""
    all_results = []
    cost_calculator = TokenCostCalculator(model_name=model_name)
    
    # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½
    cost_lock = threading.Lock()
    
    print(f"\nì´ {len(comparison_data)}ê°œ ë°°ì¹˜ë¥¼ {max_workers}ê°œ ì›Œì»¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
    
    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ê° ë°°ì¹˜ì— ëŒ€í•´ ì‘ì—… ì œì¶œ
        future_to_batch = {
            executor.submit(process_batch_with_cost_tracking, batch, cost_calculator, save_individual_batches): batch 
            for batch in comparison_data
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë“¤ì„ ìˆ˜ì§‘
        completed_count = 0
        for future in as_completed(future_to_batch):
            batch = future_to_batch[future]
            completed_count += 1
            
            try:
                result = future.result()
                if result is not None:
                    with cost_lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½
                        all_results.append(result)
                
                print(f"ì§„í–‰ë¥ : {completed_count}/{len(comparison_data)} ë°°ì¹˜ ì™„ë£Œ")
                
            except Exception as e:
                print(f"ë°°ì¹˜ {batch['batch_id']} ê²°ê³¼ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ë°°ì¹˜ ID ìˆœì„œëŒ€ë¡œ ì •ë ¬
    all_results.sort(key=lambda x: x.get('batch_id', 0))
    
    print(f"\në³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(all_results)}ê°œ ë°°ì¹˜ ì²˜ë¦¬ë¨")
    
    # ì „ì²´ ë¹„ìš© ìš”ì•½ ì¶œë ¥
    cost_calculator.print_total_cost_summary()
    
    # í† í° ë¹„ìš© ì •ë³´ë¥¼ txt íŒŒì¼ë¡œ ì €ì¥
    if save_results:
        save_cost_summary_to_file(cost_calculator, all_results)
    
    if save_results and all_results:
        if use_structured_export:
            # ìƒˆë¡œìš´ êµ¬ì¡°í™”ëœ ë‚´ë³´ë‚´ê¸° ì‚¬ìš© (comparison_data ì „ë‹¬)
            export_structured_results(all_results, comparison_data)
            
            # ìš”ì•½ ë¦¬í¬íŠ¸ ì¶œë ¥
            print(generate_analysis_report(all_results))
        else:
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì €ì¥
            save_batch_results(all_results)
    
    return all_results

def save_results_to_original_location(all_results, comparison_data, source_file_path):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ì›ë³¸ íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì— translation_review_llm.csvë¡œ ì €ì¥"""
    if not all_results:
        print(f"ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤: {source_file_path}")
        return None
    
    # ì›ë³¸ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    source_dir = Path(source_file_path).parent
    output_csv_path = source_dir / "translation_review_llm.csv"
    
    # CSV í˜•íƒœë¡œ ê° ìŒë³„ ê²°ê³¼ ì €ì¥
    csv_data = []
    
    # ë°°ì¹˜ë³„ë¡œ ì›ë³¸ ë°ì´í„°ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ë§¤ì¹­
    for batch in comparison_data:
        batch_id = batch['batch_id']
        original_items = batch['data']
        
        # í•´ë‹¹ ë°°ì¹˜ì˜ ë¶„ì„ ê²°ê³¼ ì°¾ê¸°
        batch_analysis = None
        for result in all_results:
            if result['batch_id'] == batch_id:
                batch_analysis = result
                break
        
        if not batch_analysis:
            # ë¶„ì„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ì›ë³¸ ë°ì´í„°ëŠ” ë³´ì¡´
            for original_item in original_items:
                csv_row = {
                    'file_name': original_item.get('file_name', ''),
                    'text_box_order': original_item.get('text_box_order', ''),
                    'target': original_item.get('target', ''),
                    'val': original_item.get('val', ''),
                    'tag': '',
                    'comment': 'ë¶„ì„ ì‹¤íŒ¨'
                }
                csv_data.append(csv_row)
            continue
        
        analysis_results = batch_analysis.get('analysis_result', []) if batch_analysis.get('parsing_success', False) else []
        
        # ì›ë³¸ ë°ì´í„°ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ìˆœì„œëŒ€ë¡œ ë§¤ì¹­
        for i, original_item in enumerate(original_items):
            analysis_item = analysis_results[i] if i < len(analysis_results) else {}
            
            csv_row = {
                'file_name': original_item.get('file_name', ''),
                'text_box_order': original_item.get('text_box_order', ''),
                'target': original_item.get('target', ''),
                'val': original_item.get('val', ''),
                'tag': ', '.join(analysis_item.get('tag', [])) if analysis_item else '',
                'comment': analysis_item.get('comment', '') if analysis_item else 'ë¶„ì„ ê²°ê³¼ ì—†ìŒ'
            }
            csv_data.append(csv_row)
    
    if csv_data:
        # CSV íŒŒì¼ ì €ì¥
        df = pd.DataFrame(csv_data)
        df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   - CSV: {output_csv_path}")
        print(f"   - ì´ {len(csv_data)}ê°œ í•­ëª© ì €ì¥")
        
        return {
            'csv_path': str(output_csv_path)
        }
    
    return None

def process_multiple_csv_files(base_path, guideline_path=None, model_name="gemini-2.5-pro", max_workers=10):
    """ì—¬ëŸ¬ translation_review.csv íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
    print(f"\n{'='*80}")
    print(f"ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
    print(f"ê¸°ë³¸ ê²½ë¡œ: {base_path}")
    if guideline_path:
        print(f"ê°€ì´ë“œë¼ì¸ íŒŒì¼: {guideline_path}")
    print(f"{'='*80}")
    
    # ëª¨ë“  translation_review.csv íŒŒì¼ ì°¾ê¸°
    csv_files = find_translation_review_files(base_path)
    
    if not csv_files:
        print(f"'{base_path}' ê²½ë¡œì—ì„œ translation_review.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"ë°œê²¬ëœ íŒŒì¼ ëª©ë¡:")
    for i, csv_file in enumerate(csv_files, 1):
        print(f"  {i}. {csv_file}")
    
    # ê° íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    all_file_results = []
    
    print(f"\nì´ {len(csv_files)}ê°œ íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    
    with ThreadPoolExecutor(max_workers=min(len(csv_files), max_workers)) as executor:
        # ê° íŒŒì¼ì— ëŒ€í•´ ì‘ì—… ì œì¶œ
        future_to_file = {
            executor.submit(process_single_csv_file, csv_file, guideline_path, model_name, max_workers): csv_file 
            for csv_file in csv_files
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë“¤ì„ ìˆ˜ì§‘
        completed_count = 0
        for future in as_completed(future_to_file):
            csv_file = future_to_file[future]
            completed_count += 1
            
            try:
                result = future.result()
                if result is not None:
                    all_file_results.append(result)
                
                print(f"\níŒŒì¼ ì²˜ë¦¬ ì§„í–‰ë¥ : {completed_count}/{len(csv_files)} ì™„ë£Œ")
                
            except Exception as e:
                print(f"íŒŒì¼ '{csv_file}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                all_file_results.append({
                    'source_file': csv_file,
                    'error': str(e),
                    'success': False
                })
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*80}")
    print(f"ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"{'='*80}")
    
    successful_files = [r for r in all_file_results if r.get('success', False)]
    failed_files = [r for r in all_file_results if not r.get('success', False)]
    
    print(f"ì„±ê³µí•œ íŒŒì¼: {len(successful_files)}ê°œ")
    print(f"ì‹¤íŒ¨í•œ íŒŒì¼: {len(failed_files)}ê°œ")
    
    if successful_files:
        print(f"\nì„±ê³µí•œ íŒŒì¼ë“¤:")
        for result in successful_files:
            file_name = Path(result['source_file']).name
            print(f"  âœ… {file_name} - {result['total_batches']}ê°œ ë°°ì¹˜ ì²˜ë¦¬ë¨")
    
    if failed_files:
        print(f"\nì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
        for result in failed_files:
            file_name = Path(result['source_file']).name
            print(f"  âŒ {file_name} - ì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    return all_file_results

def get_summary_statistics(all_results):
    """ì „ì²´ ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ìš”ì•½ í†µê³„"""
    if not all_results:
        return "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    total_batches = len(all_results)
    total_items = sum(result['processed_items'] for result in all_results)
    
    summary = f"""
=== ë¶„ì„ ìš”ì•½ í†µê³„ ===
- ì²˜ë¦¬ëœ ì´ ë°°ì¹˜ ìˆ˜: {total_batches}
- ì²˜ë¦¬ëœ ì´ í•­ëª© ìˆ˜: {total_items}
- ë°°ì¹˜ë‹¹ í‰ê·  í•­ëª© ìˆ˜: {total_items/total_batches:.1f}
"""
    return summary

def find_preprocessed_directory(project_uuid, episode_number):
    """project_uuidì™€ episode_numberë¥¼ ê¸°ë°˜ìœ¼ë¡œ preprocessed ë””ë ‰í† ë¦¬ ê²½ë¡œ ì°¾ê¸°"""
    # ê¸°ë³¸ data ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
    base_data_dir = "../data"
    project_dir = os.path.join(base_data_dir, project_uuid)
    
    if not os.path.exists(project_dir):
        print(f"âŒ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {project_dir}")
        return None
    
    # episode_number ë””ë ‰í† ë¦¬ ì°¾ê¸°
    episode_dir = os.path.join(project_dir, episode_number)
    if not os.path.exists(episode_dir):
        print(f"âŒ ì—í”¼ì†Œë“œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {episode_dir}")
        return None
    
    # preprocessed ë””ë ‰í† ë¦¬ ì°¾ê¸°
    preprocessed_dir = os.path.join(episode_dir, "preprocessed")
    if not os.path.exists(preprocessed_dir):
        print(f"âŒ preprocessed ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {preprocessed_dir}")
        return None
    
    return preprocessed_dir

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª…ë ¹í–‰ ì¸ì í™•ì¸
    if len(sys.argv) != 4:
        print("âŒ ì‚¬ìš©ë²•: python3 4_comment_generator.py <project_uuid> <episode_number> <guideline_path>")
        print("   guideline_pathê°€ ë¹ˆ ê°’ì´ë©´ 'none'ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”")
        return
    
    project_uuid = sys.argv[1]
    episode_number = sys.argv[2]
    guideline_path = sys.argv[3] if sys.argv[3] != 'none' and sys.argv[3] != '' else None
    
    print(f"ğŸ” í”„ë¡œì íŠ¸ UUID: {project_uuid}")
    print(f"ğŸ” ì—í”¼ì†Œë“œ ë²ˆí˜¸: {episode_number}")
    print(f"ğŸ“‹ ê°€ì´ë“œë¼ì¸ íŒŒì¼: {guideline_path if guideline_path else 'ì—†ìŒ'}")
    
    # preprocessed ë””ë ‰í† ë¦¬ ìë™ ê²€ìƒ‰
    base_path = find_preprocessed_directory(project_uuid, episode_number)
    if not base_path:
        return
    
    print(f"ğŸ“ ê¸°ë³¸ ê²½ë¡œ: {base_path}")
    
    print("\n" + "="*50)
    print("ë‹¤ì¤‘ íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ:")
    print("="*50)

    # ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬ ì‹¤í–‰
    all_file_results = process_multiple_csv_files(
        base_path=base_path,
        guideline_path=guideline_path,
        model_name="gemini-2.5-pro",
        max_workers=5  # íŒŒì¼ ìˆ˜ì¤€ ë³‘ë ¬ì„± (ê° íŒŒì¼ ë‚´ì—ì„œë„ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ë¨)
    )
    
    # ì „ì²´ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
    if all_file_results:
        print(f"\n{'='*80}")
        print(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½")
        print(f"{'='*80}")
        
        successful_files = [r for r in all_file_results if r.get('success', False)]
        total_batches = sum(r.get('total_batches', 0) for r in successful_files)
        
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œëœ íŒŒì¼: {len(successful_files)}ê°œ")
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ëœ ë°°ì¹˜: {total_batches}ê°œ")
        
        # ê° íŒŒì¼ë³„ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜ í‘œì‹œ
        print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥ ìœ„ì¹˜:")
        for result in successful_files:
            source_path = Path(result['source_file'])
            csv_path = source_path.parent / "translation_review_llm.csv"
            print(f"  - {source_path.name} â†’ {csv_path}")
        
        print(f"\nğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ê° ì›ë³¸ íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì— 'translation_review_llm.csv' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()